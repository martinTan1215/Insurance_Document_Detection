{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNH5Sjb44b61/A7URH4MqiJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinTan1215/Insurance_Document_Detection/blob/main/InsuranceDocumentDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training\n"
      ],
      "metadata": {
        "id": "yBM99WhnXFZ3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXKzFbLEW2lq"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import pytesseract\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, I import the necessary libraries and modules that will be used in the code. This includes the PIL library for image processing, numpy for numerical operations, os for file and directory operations, pytesseract for text extraction from images, and scikit-learn modules for data preprocessing, model training, and evaluation. I also import the joblib module for saving and loading the trained model."
      ],
      "metadata": {
        "id": "vIYdQfIPW7yJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to your images\n",
        "image_dir = '/Users/tanqiao/Downloads/Insurance_Dataset'\n",
        "\n",
        "# Set the names of your classes\n",
        "classes = ['Home Insurance Claim Form', 'Car Insurance Claim Form',\n",
        "           'Health Insurance Claim Form']"
      ],
      "metadata": {
        "id": "KZB0OH4FXBjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, I specify the path to the directory where the images are stored. You need to replace the path with the actual path on your system. I also define the names of the classes, which represent the different types of insurance claim forms."
      ],
      "metadata": {
        "id": "9ds45YnMXNzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create lists to store the data and labels\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "\n",
        "# Loop over each class\n",
        "for label, class_name in enumerate(classes):\n",
        "    # Set the path to the class folder\n",
        "    class_dir = os.path.join(image_dir, class_name)\n",
        "\n",
        "    # Loop over each image in the class folder\n",
        "    for filename in os.listdir(class_dir):\n",
        "        # Open the image\n",
        "        image = Image.open(os.path.join(class_dir, filename))\n",
        "\n",
        "        # Resize the image pixels\n",
        "        image = image.resize((500, 500))\n",
        "\n",
        "        # Convert the image to grayscale\n",
        "        image = image.convert('L')\n",
        "\n",
        "        # Use pytesseract to convert image to text\n",
        "        text = pytesseract.image_to_string(image)\n",
        "\n",
        "        # Replace 'Auto Insurance', 'Vehicle Insurance' with 'Car Insurance'\n",
        "        text = text.replace('Auto Insurance Claim Form',\n",
        "                            'Car Insurance Claim Form')\n",
        "        text = text.replace('Auto Insurance', 'Car Insurance Claim Form')\n",
        "        text = text.replace('Vehicle Insurance Claim Form',\n",
        "                            'Car Insurance Claim Form')\n",
        "        text = text.replace('Vehicle Insurance', 'Car Insurance Claim Form')\n",
        "        text = text.replace('Motor Insurance', 'Car Insurance Claim Form')\n",
        "\n",
        "        # Replace 'Property Insurance' with 'Home Insurance'\n",
        "        text = text.replace('Property Insurance Claim Form',\n",
        "                            'Home Insurance Claim Form')\n",
        "        text = text.replace('Property Insurance', 'Home Insurance Claim Form')\n",
        "\n",
        "        # Replace 'Health Insurance' with 'Health Insurance'\n",
        "        text = text.replace('Health Insurance', 'Health Insurance Claim Form')\n",
        "        text = text.replace('Life Insurance', 'Health Insurance Claim Form')\n",
        "\n",
        "        # Add the text to the data list\n",
        "        x_data.append(text)\n",
        "\n",
        "        # Add the label to the labels list\n",
        "        y_data.append(label)"
      ],
      "metadata": {
        "id": "VygRYwpsXXLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section is responsible for preparing the data and labels for training the model. I initialize two lists, x_data and y_data, to store the text data and corresponding labels. I loop over each class and each image in the class folder. For each image, I open it, resize it to 300x300 pixels, convert it to grayscale, and extract the text using pytesseract. I perform some text replacements to standardize the class names. The extracted text is added to x_data, and the label (represented by the index of the class) is added to y_data."
      ],
      "metadata": {
        "id": "CjUfOa9WXY3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the data and labels to NumPy arrays\n",
        "x_data = np.array(x_data)\n",
        "y_data = np.array(y_data)\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = TfidfVectorizer()\n",
        "x_data = vectorizer.fit_transform(x_data)\n",
        "\n",
        "# Save the fitted vectorizer\n",
        "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")"
      ],
      "metadata": {
        "id": "1NZkZLgqXeGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, I convert the data and labels into NumPy arrays for further processing. I then use the TfidfVectorizer from scikit-learn to convert the text data into a numerical representation using TF-IDF (Term Frequency-Inverse Document Frequency) encoding. The vectorizer is fitted on the text data using the fit_transform() method. I also save the fitted vectorizer for later use in prediction."
      ],
      "metadata": {
        "id": "gzalQ2YjXgEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x_data, y_data, test_size=0.2)"
      ],
      "metadata": {
        "id": "b2_fywrfXj_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section splits the data and labels into training and testing sets using the train_test_split() function from scikit-learn. The test_size parameter specifies the proportion of the data to be allocated for testing (in this case, 20%)."
      ],
      "metadata": {
        "id": "JIJKb8Z9XluU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train a classifier\n",
        "clf = SVC()\n",
        "\n",
        "# Perform cross-validation on the training data\n",
        "cv_scores = cross_val_score(clf, x_train, y_train, cv=5)\n",
        "\n",
        "# Fit the classifier on the training data\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "# Evaluate the classifier on the training set\n",
        "train_accuracy = clf.score(x_train, y_train)\n",
        "\n",
        "# Evaluate the classifier on the test set\n",
        "test_accuracy = clf.score(x_test, y_test)\n",
        "\n",
        "# Print the cross-validation scores and performance metrics\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Mean cross-validation score:\", np.mean(cv_scores))\n",
        "print(\"Train accuracy:\", train_accuracy)\n",
        "print(\"Test accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "Lq_NN__RXov8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, I create an SVM (Support Vector Machine) classifier using the SVC class from scikit-learn. I perform cross-validation on the training data using the cross_val_score() function, with 5-fold cross-validation. This provides an estimate of the model's performance on unseen data. I then fit the classifier on the training data using the fit() method. The classifier's accuracy is evaluated on both the training set (train_accuracy) and the test set (test_accuracy). Finally, it will print the cross-validation scores and performance metrics to evaluate the model's performance."
      ],
      "metadata": {
        "id": "5I-9z8RVXqgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "joblib.dump(clf, \"insurance_classification_model.pkl\")"
      ],
      "metadata": {
        "id": "6kKHPwDmXxNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we save the trained classifier using the joblib.dump() function from the joblib module. The trained model is saved in a file named \"insurance_classification_model.pkl\". This allows us to load and use the trained model for making predictions in the future.\n",
        "\n",
        "Overall, the entire code of Model Training performs the following steps:\n",
        "\n",
        "\n",
        "1.   Reads and preprocesses the image data.\n",
        "2.   Extracts the text from the images using OCR.\n",
        "3.   Standardizes the class names in the text data.\n",
        "4.   Converts the text data into numerical features using TF-IDF encoding.\n",
        "5.   Splits the data into training and testing sets.\n",
        "6.   Trains an SVM classifier on the training data and evaluates its performance.\n",
        "7.   Saves the trained model for future use."
      ],
      "metadata": {
        "id": "7m5qmWJwX5tY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ],
      "metadata": {
        "id": "TqGs4ENqYS7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import os\n",
        "import shutil"
      ],
      "metadata": {
        "id": "EM4TzjvJYVq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, I import the necessary libraries for image processing, text extraction, file operations, and copying files."
      ],
      "metadata": {
        "id": "d99bjCv2YYhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "clf = joblib.load(\"insurance_classification_model.pkl\")\n",
        "\n",
        "# Load the fitted vectorizer\n",
        "vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")"
      ],
      "metadata": {
        "id": "rEhja0UBYj1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I load the previously trained model (saved as insurance_classification_model.pkl) and the fitted vectorizer (saved as tfidf_vectorizer.pkl) using the joblib library."
      ],
      "metadata": {
        "id": "Ar84Ewg5Yllc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to the folder containing the images to predict\n",
        "folder_path = \"/Users/tanqiao/Desktop/Test_Dataset\"\n",
        "\n",
        "# Set the paths for the output folders\n",
        "output_folder_home = \"/Users/tanqiao/Desktop/DestinationFolder/Home\"\n",
        "output_folder_car = \"/Users/tanqiao/Desktop/DestinationFolder/Car\"\n",
        "output_folder_health = \"/Users/tanqiao/Desktop/DestinationFolder/Health\"\n",
        "\n",
        "# Create the output folders if they don't exist\n",
        "os.makedirs(output_folder_home, exist_ok=True)\n",
        "os.makedirs(output_folder_car, exist_ok=True)\n",
        "os.makedirs(output_folder_health, exist_ok=True)"
      ],
      "metadata": {
        "id": "3tiFp72VYlrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, I set the path to the folder containing the images to predict (folder_path). I also set the paths for the output folders (output_folder_home, output_folder_car, output_folder_health). If the output folders don't exist, they are created using the os.makedirs function"
      ],
      "metadata": {
        "id": "zt2dztAWYn8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the images in the folder\n",
        "for file_name in os.listdir(folder_path):\n",
        "    # Construct the full path to the image\n",
        "    image_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "    # Open the image\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    # Resize the image pixels\n",
        "    image = image.resize((500, 500))\n",
        "\n",
        "    # Convert the image to grayscale\n",
        "    image = image.convert('L')\n",
        "\n",
        "    # Use pytesseract to convert the image to text\n",
        "    text = pytesseract.image_to_string(image)\n",
        "\n",
        "    # Preprocess the text\n",
        "    text = text.replace('Auto Insurance Claim Form', 'Car Insurance Claim Form')\n",
        "    text = text.replace('Auto Insurance', 'Car Insurance Claim Form')\n",
        "    text = text.replace('Vehicle Insurance Claim Form',\n",
        "                        'Car Insurance Claim Form')\n",
        "    text = text.replace('Vehicle Insurance', 'Car Insurance Claim Form')\n",
        "    text = text.replace('Motor Insurance', 'Car Insurance Claim Form')\n",
        "    text = text.replace('Property Insurance Claim Form',\n",
        "                        'Home Insurance Claim Form')\n",
        "    text = text.replace('Property Insurance', 'Home Insurance Claim Form')\n",
        "    text = text.replace('Health Insurance', 'Health Insurance Claim Form')\n",
        "    text = text.replace('Life Insurance', 'Health Insurance Claim Form')\n",
        "\n",
        "    # Vectorize the preprocessed text using the fitted vectorizer\n",
        "    x_new = vectorizer.transform([text])\n",
        "\n",
        "    # Make the prediction\n",
        "    predicted_label = clf.predict(x_new)[0]\n",
        "    class_names = ['Home Insurance Claim Form', 'Car Insurance Claim Form',\n",
        "                   'Health Insurance Claim Form']\n",
        "    predicted_class = class_names[predicted_label]\n",
        "\n",
        "    # Define the destination folder based on the predicted class\n",
        "    if predicted_class == 'Home Insurance Claim Form':\n",
        "        destination_folder = output_folder_home\n",
        "    elif predicted_class == 'Car Insurance Claim Form':\n",
        "        destination_folder = output_folder_car\n",
        "    elif predicted_class == 'Health Insurance Claim Form':\n",
        "        destination_folder = output_folder_health\n",
        "\n",
        "    # Copy the image to the corresponding destination folder\n",
        "    shutil.copy(image_path, os.path.join(destination_folder, file_name))\n",
        "\n",
        "    # Print the predicted class and destination folder for current image\n",
        "    print(f\"Predicted class for {file_name}: {predicted_class}\")\n",
        "    print(f\"Destination folder: {destination_folder}\")"
      ],
      "metadata": {
        "id": "2KUx1L8WYndB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section iterates over each image in the specified folder. It performs image processing steps such as resizing the image to 500x500 pixels and converting it to grayscale. The image is then converted to text using pytesseract. The text is preprocessed by replacing specific terms to match the trained model's class labels. The preprocessed text is vectorized using the fitted vectorizer. The model predicts the class label for the image, and based on the predicted class, the corresponding destination folder is defined. The image is then copied to the appropriate destination folder using shutil. Finally, the predicted class and the destination folder for each image are printed.\n",
        "\n",
        "In summary, this code loads the trained model and vectorizer, processes images in a specified folder, extracts text from the images using pytesseract, preprocesses the text, and predicts the class label for each image. The predicted images are then copied to the respective output folders based on the predicted class"
      ],
      "metadata": {
        "id": "SPfTabKaYspb"
      }
    }
  ]
}